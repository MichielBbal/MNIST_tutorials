{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ec8d034",
   "metadata": {},
   "source": [
    "# Analysing MNIST with CLIP \n",
    "\n",
    "Please note: do not use ```pip install clip``` as this is a different library!\n",
    "\n",
    "### Contents\n",
    "0. Install package\n",
    "1. Load model & run dummy script\n",
    "2. Running an image from the MNIST dataset¶\n",
    "\n",
    "### Sources: \n",
    "- https://archive.ph/XRFjo An introduction to CLIP on Medium\n",
    "- https://github.com/openai/CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31524d61",
   "metadata": {},
   "source": [
    "## 0. Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42442777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /private/var/folders/cj/qtbz9fvd3svc0x28yv2756mh0000gn/T/pip-req-build-fn13pm6d\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /private/var/folders/cj/qtbz9fvd3svc0x28yv2756mh0000gn/T/pip-req-build-fn13pm6d\n",
      "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: ftfy in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from clip==1.0) (6.1.1)\n",
      "Requirement already satisfied: regex in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from clip==1.0) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from clip==1.0) (4.61.2)\n",
      "Requirement already satisfied: torch in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from clip==1.0) (2.0.0)\n",
      "Requirement already satisfied: torchvision in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from clip==1.0) (0.15.1)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from ftfy->clip==1.0) (0.2.5)\n",
      "Requirement already satisfied: filelock in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from torch->clip==1.0) (3.9.0)\n",
      "Requirement already satisfied: networkx in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from torch->clip==1.0) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from torch->clip==1.0) (3.1.2)\n",
      "Requirement already satisfied: sympy in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from torch->clip==1.0) (1.11.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from torch->clip==1.0) (4.4.0)\n",
      "Requirement already satisfied: numpy in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from torchvision->clip==1.0) (1.23.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from torchvision->clip==1.0) (9.4.0)\n",
      "Requirement already satisfied: requests in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from torchvision->clip==1.0) (2.25.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from jinja2->torch->clip==1.0) (2.1.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages/mpmath-1.2.1-py3.10.egg (from sympy->torch->clip==1.0) (1.2.1)\n",
      "Building wheels for collected packages: clip\n",
      "  Building wheel for clip (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369497 sha256=2f5aa2a84c876bb251d031df6d1e8fdd14ead36f5e70e1973a1c90863d458a5d\n",
      "  Stored in directory: /private/var/folders/cj/qtbz9fvd3svc0x28yv2756mh0000gn/T/pip-ephem-wheel-cache-5dximt8c/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
      "Successfully built clip\n",
      "Installing collected packages: clip\n",
      "Successfully installed clip-1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60375088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.9 (main, Mar  1 2023, 12:33:47) [Clang 14.0.6 ]\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "print(sys. version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2d8beff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the conda libraries are recognized here\n",
    "import sys\n",
    "\n",
    "_ = sys.path.append(\"/usr/local/lib/python3.10/site-packages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f75ce3",
   "metadata": {},
   "source": [
    "## 1. Load model & run dummy script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e18986fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 338M/338M [00:42<00:00, 8.23MiB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# load model and image preprocessing\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "112377a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'available_models', 'clip', 'load', 'model', 'simple_tokenizer', 'tokenize']\n"
     ]
    }
   ],
   "source": [
    "#inspect clip methods\n",
    "print(dir(clip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd85d065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "\n",
      "Top predictions:\n",
      "\n",
      "           snake: 65.31%\n",
      "          turtle: 12.29%\n",
      "    sweet_pepper: 3.83%\n",
      "          lizard: 1.88%\n",
      "       crocodile: 1.75%\n"
     ]
    }
   ],
   "source": [
    "# Run a dummy script\n",
    "#source: https://github.com/openai/CLIP\n",
    "import os\n",
    "import clip\n",
    "import torch\n",
    "from torchvision.datasets import CIFAR100\n",
    "\n",
    "# Load the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load('ViT-B/32', device)\n",
    "\n",
    "# Download the dataset\n",
    "cifar100 = CIFAR100(root=os.path.expanduser(\"~/.cache\"), download=True, train=False)\n",
    "\n",
    "# Prepare the inputs\n",
    "image, class_id = cifar100[3637]\n",
    "image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in cifar100.classes]).to(device)\n",
    "\n",
    "# Calculate features\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image_input)\n",
    "    text_features = model.encode_text(text_inputs)\n",
    "\n",
    "# Pick the top 5 most similar labels for the image\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "values, indices = similarity[0].topk(5)\n",
    "\n",
    "# Print the result\n",
    "print(\"\\nTop predictions:\\n\")\n",
    "for value, index in zip(values, indices):\n",
    "    print(f\"{cifar100.classes[index]:>16s}: {100 * value.item():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecba65a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the image. Note: CIFAR images are very small\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9c2fe8",
   "metadata": {},
   "source": [
    "## 2. Running an image from the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "996f2c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
